# Python-Crawlers
Contains various python crawlers from basic to advanced level.

Each crawler has its own Documentation.

## Crawlers Name and Description
1. it_books : Basic Crawler.
2. multi_page : Recursive scraping with SgmlLinkExtractor, Rules and CrawlSpider.
3. depth_scrape : Recursive scraping to a depth of 4, with FilesPipline to download files and Selenium for javascript.
  - In this crawler I get links from within the website and redirect scrapy to these links and at the last link scrape the data.
  - For more details please see the documentation.

## Resources
The following websites and links were used as reference resources as the crawlers were built.

*[Python web scraping resource](http://jakeaustwick.me/python-web-scraping-resource/#thesiteusesajaxicantscrapeit)

*[Articles & blog posts : Guides contributed by the Scrapy Community.](https://github.com/scrapy/scrapy/wiki)
*The above repo has all the best guides. Explore it for sure.*

*[Web Scraping With Scrapy](http://www.hackhowtofaq.com/blog/web-scraping-with-scrapy/)

*[Recursive Scraping](https://pypi.python.org/pypi/scrapy-inline-requests)

*[Scrape multi-pages content with Scrapy](http://abuhijleh.net/2011/02/13/guide-scrape-multi-pages-content-with-scrapy/)

*[Recursively Scraping Web Pages With Scrapy](http://mherman.org/blog/2012/11/08/recursively-scraping-web-pages-with-scrapy/#.VQBBz817h5S)

*[File Pipline in Scrapy](https://groups.google.com/forum/#!topic/scrapy-users/kzGHFjXywuY)
